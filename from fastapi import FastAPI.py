from fastapi import FastAPI
from pydantic import BaseModel
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

# ------------------------
# Step 1: Setup FastAPI
# ------------------------
app = FastAPI()

# ------------------------
# Step 2: Load model + tokenizer
# ------------------------
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")
model.eval()

# ------------------------
# Step 3: Input Schema
# ------------------------
class TextIn(BaseModel):
    prompt: str
    max_length: int = 30

# ------------------------
# Step 4: API Endpoint
# ------------------------
@app.post("/generate")
def generate_text(data: TextIn):
    # Encode input text into tokens
    inputs = tokenizer(data.prompt, return_tensors="pt")

    # Generate output tokens
    outputs = model.generate(
        **inputs,
        max_length=data.max_length,
        num_return_sequences=1,
        do_sample=True,
       
    )

    # Decode tokens back into text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"prompt": data.prompt, "response": generated_text}

# ------------------------
# Step 5: Run the API
# ------------------------
# Save this as `app.py` and run:
# uvicorn app:app --reload
#
# Then send a POST request with JSON:
# { "prompt": "The future of AI is", "max_length": 40 }
#
# Youâ€™ll get back text generated by GPT-2.

